# 一、 引言(Introduction)

## 1.1 欢迎
- What is machine learning? 你可能每天用到它许多次却不知道它。 
- 每次你用谷歌或者必应搜索，它们很好用 是因为它们的机器学习软件已经计算出如何对网页排名 脸书或者苹果的照片应用在照片中识别出你的朋友， 这也是机器学习。
-  每次你阅读你的邮件 垃圾邮件过滤器把你从海量的垃圾邮件中解救出来， 这也是因为你的计算机学习了如何从非垃圾邮件中辨别垃圾邮件。 这就是机器学习。 这是一门是让计算机 无需显式编程 (explicitly programmed) 就能自主学习的学科。 
-  我正在做的其中一个研究项目是让机器人去 整理房间。 你要怎样去做这个？ 你能做的是使机器人看你的演示任务和 从中学习。 机器人会看你拿起了什么东西，放在了哪里 试着去做同样的事情，即使在你不在的时候。 
-  但在大多数情况下 我们并不知道如何显式地编写人工智能程序 来做一些更有趣的任务 比如网页搜索 标记照片和拦截垃圾邮件等 人们意识到唯一能够达成这些目标的方法 就是让机器自己学会如何去做 因而 机器学习已经发展成为计算机的一项新能力
- 在硅谷 机器学习引导着大量的课题 如自主机器人、计算生物学等 机器学习的实例还有很多 比如数据库挖掘
## 1.2 机器学习是什么？
- 下面是什么是机器学习的定义，因为亚瑟·塞缪尔。 他 将机器学习定义为赋予计算机学习能力而无需明确学习的研究领域。
- He says, a computer program is said to learn from experience E with respect to some task T and some performance measure P, if its performance on T, as measured by P, improves with experience E.
```
例子：下棋
E = 玩多场跳棋的经验
T = 下棋的任务。
P = 程序赢得下一场比赛的概率
```
- 事实证明，在监督学习中， 我们的想法是要教计算机如何做一些事情。 而在无监督的学习中，我们将让它自己学习
- 算法很重要，如何使用这些算法同样重要
## 1.3 监督学习
- 监督学习是指 事实上 我们给算法的数据集 被称为“正确答案” 
- 在监督学习中，我们得到了一个数据集，并且已经知道我们的正确输出应该是什么样子，并且知道输入和输出之间存在关系。
- 监督学习问题分为“回归”和“分类”问题。在回归问题中，我们试图预测连续输出中的结果，这意味着我们试图将输入变量映射到某个连续函数。在分类问题中，我们试图在离散输出中预测结果。换句话说，我们试图将输入变量映射到离散类别中。
- (a) 回归——给定一张人的照片，我们必须根据给定的照片预测他们的年龄
- (b) 分类——给定一个患有肿瘤的患者，我们必须预测肿瘤是恶性的还是良性的。 
- 当我们试图预测的目标变量是连续的时，例如在我们的住房示例中，我们将学习问题称为回归问题。当 y 只能采用少量离散值时（例如，如果在给定居住面积的情况下，我们想预测住宅是房屋还是公寓），我们将其称为分类问题。
## 1.4 无监督学习
```
谷歌新闻每天都在干什么呢？
从 :1:23 开始播放视频并学习脚本1:23
他们每天会去收集 成千上万的 网络上的新闻 然后将他们分组 组成一个个新闻专题
```
- 没有正确答案，例如聚类算法。
- 聚类：收集 1,000,000 个不同的基因，并找到一种方法将这些基因自动分组到不同的变量（如寿命、位置、角色等）之间，这些基因在某种程度上相似或相关。
- 比如，基因的分类，社交网络的分析，客户的细分
- 非聚类：“鸡尾酒会算法”，可让您在混乱的环境中找到结构。（即从鸡尾酒会上的声音网中识别个人声音和音乐）。
```
而事实上在硅谷 很多人会这样做 他们会先用Octave 来实现这样一个学习算法原型 只有在确定 这个算法可以工作后 才开始迁移到C++ Java或其它编译环境
事实证明 这样做 实现的算法 比你一开始就用C++ 实现的算法要快多了
所以 我知道 作为一个老师 我不能老是念叨： “在这个问题上相信我“ 但对于 那些从来没有用过这种类似Octave的编程环境的童鞋  
我还是要请你 相信我这一次 我认为你的时间 研发时间是你最宝贵的资源之一
```
# 二、单变量线性回归(Linear Regression with One Variable)

## 2.1 模型表示
回归一词指的是我们根据之前的数据预测出一个准确的输出值</br>
m,x's,y's,x(i),y(i),将训练集训练算法,这就是学习算法的工作了,然后输出一个函数,按照惯例 通常表示为小写h h代表hypothesis(假设) h表示一个函数，由于历史原因，这个函数 h 被称为假设</br>
hΘ(x)=Θ0+Θ1*x，单变量线性回归模型
## 2.2 代价函数
还有其他的代价函数也能很好地发挥作用 但是平方误差代价函数可能是解决回归问题最常用的手段了。</br>
minimizeΘ0Θ1(1/2m∑(hΘ(x(i)-y(i))^2)
![image](https://user-images.githubusercontent.com/51777429/120885214-54421b80-c61a-11eb-8d36-0b79abef0467.png)
## 2.3 代价函数的直观理解I
在θ0=0的情况下，J(θ)编程了单变量的函数，实际上在这里是个个准确
## 2.4 代价函数的直观理解II

## 2.5 梯度下降

## 2.6 梯度下降的直观理解

## 2.7 梯度下降的线性回归

## 2.8 接下来的内容

# 三、线性代数回顾(Linear Algebra Review)

## 3.1 矩阵和向量

## 3.2 加法和标量乘法

## 3.3 矩阵向量乘法

## 3.4 矩阵乘法

## 3.5 矩阵乘法的性质

## 3.6 逆、转置
